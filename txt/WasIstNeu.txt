Version 1.5.1 am 07.08.08 
-------------------------
* Dateiname wird korrekt indiziert
* Datumsformat 'last-modified' auf "YYYYMMDD" geändert. Damit kann man die RangeSearch auf
  das Feld anwenden (Code-Contribution)
* locale-Handling im SharedTag verbessert (Code-Contribution)

Version 1.5.0 am 10.07.08 Preview
---------------------------------
* Linkextraktion von Regexp auf HTMLParser umgestellt
* HTMLParser jetzt komplett integriert
* -notrayicon - Kommandozeilenparameter fÃ¼r Desktopsuche (TrayIcon wird nicht angezeigt)
* Lucene auf 2.3.2 geupdated
* Anchors werden von URLs entfernt (http://forum.murfman.de/index.html#bottom --> http://forum.murfman.de/index.html)
* Definition eines default Indexupdate-Intervalles
* Ã„nderung LÃ¶schvorgang temporÃ¤re Dateien
* Verbesserte Mimetype-Detection (Update Aperture, URL-Bestimmung)

Version 1.4.2 am 04.06.08 Preview
---------------------------------
* http://-Links, welche mit einem / enden, kÃ¶nnen extrahiert und indiziert werden

Version 1.4.1 am 27.04.08 Preview
---------------------------------
* JavaPreparator fuer *.java-Dateien. Der JavaPreparator ist aufgrund seiner 
  Groesse und dem begrenzten Nutzerkreis nicht Teil der Standardistribution.


Version 1.4.0 am 05.04.08 Preview
---------------------------------
* Bugfix: StackOverflowError in Linkextraction abgefangen
* mp3-Preparator extrahiert von ID3v2 oder ID3v1 Tags
* Generischer Audio Preparator, welcher von mp4 (iTunes), ogg-Vorbis oder 
  flac die Metadaten extrahiert


Version 1.3.0/1 am 16.03.08
---------------------------

* Zugriff auf Windows-Shares (ohne Authentifizierung) (Samba/CIFS)
* HTML-Parser zum Extrahieren des Contents
* Bugfix Mimetypdetection, wenn das File keine Dateinendung besitzt
* PrioritÃ¤t fÃ¼r Preparatoren. Default Prio ist 0. Damit lÃ¤sst sich mittels
  URLRegEx .* in einem beliebigen Preparator (vorzugsweise EmptyPreparator)
  und priority=-1 ein CatchAll-Preparator konfigurieren
* Highlighting fÃ¼r Content und Titel
* Preparatorselektion anhand des Mimetypes (frÃ¼her Dateiendung)
* Mimetype-detection (Dateiendung und MagicMime) 
* Umstellung Erweiterte Suche auf Auswahl Mimetype anstelle Dateiendung


Version 1.2.3 am 01.12.07
-------------------------

* Bugfix: In manchen FÃ¤llen wurden keine Dateiinhalte indexiert.


Version 1.2.2 am 01.11.07
-------------------------

* Es kann nun jeder beliebige Lucene-Analyzer verwendet werden.
* Bugfix: In der TLD-Definition hat das Attribut beautified im hit_url-Tag gefehlt.
* Bugfix: Im Zusammenhang mit der File-to-Http-Bridge gab es URL-encoding-Probleme


Version 1.2.1 am 30.10.07
-------------------------

* Bugfix: In regain 1.2 haben ein paar Bibliotheken gefehlt. Dies wurde mit
  Version 1.2.1 behoben.


Version 1.2 am 20.10.07
-----------------------

* In den Suchergebnissen werden nun Icons gezeigt, die den Typ einer Datei
  kennzeichnen. 
* Die Index-Felder "size" und "last-modified" sind nun suchbar.
* Neuer PrÃ¤parator: EmptyPreparator (Beigesteuert von Gerhard Olsson). Dieser
  PrÃ¤parator extrahiert keinen Inhalt aus den ihm zugeordneten Dateien. Dadurch
  landet im Index nur der Pfad und Dateiname (hilfreich fÃ¼r alle Dateitypen, fÃ¼r
  die es keinen PrÃ¤parator gibt).
* Die maximale Anzahl von Termen pro Dokument ist nun einstellbar und zwar Ã¼ber
  das <maxFieldLength>-Tag in der CrawlerConfiguration.xml. Default ist 10000.
* Der IfilterPreparator funktioniert jetzt auch unter Windows Server 2003.
* Die Werte fÃ¼r das <search:input_fieldlist>-Tag kÃ¶nnen nun beim Indexieren
  ermittelt werden. Dadurch muss diese bei groÃŸen Indexen langsame Operation
  nicht mehr bei der ersten Suche gemacht werden. Dies kann Ã¼ber das
  <valuePrefetchFields>-Tag in der CrawlerConfiguration.xml eingestellt werden.
* Mehrere Bugfixes


Version 1.1.1 am 27.03.06
-------------------------

* Bugfixes in der Server-Variante


Version 1.1 am 26.02.06
-----------------------

* regain sucht nun auch in den URLs.
* Die Desktop-Suche zeigt nun die letzten Log-Meldungen.
* Bessere Behandlung von HTTP-Redirects. (Danke an Gerhard Olsson)
* Zusatzfelder haben nun die Optionen "tokenize", "store" und "index".
* Die Tag Library ist nun dokumentiert.
* Die Suchmaske akzeptiert nun mehrere "query" Parameter
  (diese werden einfach aneinandergehï¿½ngt)
* Die Jacob-PrÃ¤paratoren wurden verbessert. (Danke an Reinhard Balling)
* Neuer PrÃ¤parator ExternalPrepartor: Dieser PrÃ¤parator ruft externe Programme
  oder Skripte auf, um den Text aus Dokumenten zu extrahieren.
  (Danke an Paul Ortyl)
* Italienische Lokalisierung fertiggestellt. (Danke an Franco Lombardo)
* Ein paar Bugfixes


Version 1.1 Beta 6 am 05.12.05
------------------------------

* Neuer PrÃ¤parator: Mit dem PoiMsPowerPointPreparator steht nun auch ein
  plattformunabhï¿½ngiger PrÃ¤parator fÃ¼r Powerpoint zur VerfÃ¼gung.
  (Vielen Dank an Gerhard Olsson)
* Neuer PrÃ¤parator: Der IfilterPreparator nutzt die I-Filter-Schnittstelle von
  Microsoft um etliche Dateiformate zu lesen. Leider lÃ¤uft er nur unter Windows.
* Multiindexsuche: In der SearchConfiguration.xml kï¿½nnen nun auch mehrere
  Indizes als Default angegeben werden.
* Die Zusatzfelder (Auxiliary Fields) kÃ¶nnen nun besser mit GroÃŸ-/Kleinschreibung
  umgehen.
* Der vom Crawler an die Webserver gesendete HTTP-Agent ist nun in der
  CrawlerConfiguration.xml konfigurierbar. So kann sich der Crawler
  beispielsweise als Internet Explorer ausgeben.
* Mehrere Bugfixes


Version 1.1 Beta 5 am 13.08.05
------------------------------

* Multiindex-Suche: Es kÃ¶nnen nun mehrere Suchindizes Ã¼ber eine Suchmaske
  durchsucht werden. Die Suchanfragen werden dabei auf jeden Index losgelassen
  und anschlieÃŸend vereint.
* In der Weiï¿½en und der Schwarzen Liste kï¿½nnen jetzt auch regulï¿½re Ausdrï¿½cke
  angegeben werden.
* Suchmaske: Der Ort der Ressourcen und der Konfiguration wird jetzt besser
  erkannt, so dass regain auch dann korrekt funktioniert, wenn Tomcat als
  Service lï¿½uft.
* Suchmaske: Die File-zu-Http-Brï¿½cke ist nun abschaltbar.
* Crawler: Der Crawler braucht nun beim Durchsuchen von Verzeichnissen weniger
  Arbeitsspeicher
* Crawler: Der Crawler nimmt nun auch fehlgeschlagene Dokumente in den Index
  auf, so dass diese bei einem erneuten Durchlauf nicht noch einmal probiert
  werden. Wird der Crawler jedoch mit der Option "-retryFailedDocs" aufgerufen,
  werden alle fehlgeschlagenen erneut probiert.
* Der Html-Prï¿½parator ï¿½bernimmt nun auch die Endungen .jsp, .php, .php3, .php4
  und .asp.
* In der CrawlerConfiguration.xml kann nun bei einem Prï¿½parator angegeben
  werden, welche Dokumente er prï¿½parieren soll.
* Mehrere Bugfixes


Version 1.1 Beta 4 am 13.04.05
------------------------------

* Zugriffsrechte-Management: Es kann nun ein Rechte-Management eingebunden
  werden, das dafuer sorgt, dass ein Benutzer nur Treffer fï¿½r Dokumente erhaelt,
  fï¿½r die er Leserechte hat.
* Suche: Die search-Taglib hat nun ein Tag "hit_field", das ein beliebiges
  Indexfeld ausgibt. Das Tag "hit_summary" wurde in diesem Zuge entfernt.
* Suche: Wenn Sie die Konfiguration der Suche nicht von einer XML-Datei laden
  wollen oder wenn Sie die Lage der XML-Datei nicht in der web.xml angeben
  moechten, koennen Sie die Konfiguration nun ueber eine eigene Factory-Klasse
  erzeugen. Die SearchConfigFactory-Klasse wird in der web.xml festgelegt.
* Serversuche: Die beigelegten JSP-Seiten haben nicht funktioniert.


Version 1.1 Beta 3 am 17.03.05
------------------------------

* Crawler: Bugfix: Der PoiMsExcelPraeparator kam nicht mit allen Zahlen- und
  Datumsformaten klar.
* Crawler: Das Fehler-Log im Index ist nun ausfuehrlicher (Mit Stacktrace).
* Crawler: Die Praeparatoren sind nun in eigene Jars gekapselt. Dadurch ist im
  regain.jar nur das, was regain selbst braucht und die Praeparatoren lassen
  sich leichter austauschen. Ausserdem koennen nun andere Entwickler auch
  Praeparatoren anbieten, die sehr einfach eingebunden werden koennen.
  Die Konfiguration der Praeparatoren steckt weiterhin in der
  CrawlerConfiguration.xml, allerdings muessen dort nicht mehr alle
  Praeparatoren angegeben werden. Die Praeparatoren werden in der Reihenfolge
  abgearbeitet, in der sie konfiguriert sind, die nicht konfigurierten
  Praeparatoren in unbestimmter Reihenfolge danach.
* Desktopsuche: Die Desktopsuche lauft nun auch unter Linux.
* Suche: Bugfix: Dateien, deren URL ein doppelter Slash enthielt
  (Z.B. bei Netzlaufwerken: //fileserver/bla/blubb) konnten nicht geladen werden.
* Desktopsuche: Bugfix: Bei den Suchergebnissen wurden Umlaute falsch
  dargestellt.
* Desktopsuche: In der Statusseite kann nun eine laufende Indexierung angehalten
  und eine Indexierung manuell gestartet werden.
* Crawler: Bugfix: Der HtmlPraeparator kam nicht mit allen Dateien klar.


Version 1.1 Beta 2 am 12.03.05
------------------------------

* Crawler: Der Crawler erstellt nun regelmaeï¿½ig sog. Breakpoints. Dabei wird
  der aktuellen Stand der Suchindex in ein gesondertes Verzeichnis kopiert.
  Falls die Indexierung abgebrochen wurde (Z.B. weil der Rechner
  heruntergefahren wurde), wird beim naechsten mal auf dem letzten Breakpoint
  aufgesetzt.
* Desktopsuche: Die Statusseite zeigt nun auch die Zeitmessungsergebnisse.


Version 1.1 Beta 1 am 10.03.05 
------------------------------

* Desktopsuche: regain bietet nun neben der Server-Suche auch eine Desktop-Suche.
  Die Desktop-Suche bietet viele Eigenschaften, die die Bedienung kinderleicht
  machen:
    - Ein Installer fuer Windows.
    - Integration in die Taskleiste unter Linux und Windows.
    - Konfiguration ueber den Browser.
    - Status-Anzeige ueber den Browser.
* Crawler: Es gibt nun einen Praeparator fuer OpenOffice- und StarOffice-Dokumente.
* Gesamt: Aktualisierung auf die neusten Versionen der genutzten Projekte.
* Crawler: Praeparatoren sind nun ueber die CrawlerConfiguration.xml
  konfigurierbar.
* Suche: Die Suche wird nun ueber die SearchConfiguration.xml
  konfiguriert, nicht mehr ueber die web.xml. Dort steht nun nur noch der Pfad
  zur SearchConfiguration.xml.
* Suche: In der Suche kann nun URL-Rewriting eingesetzt werden. Dadurch
  koennen Dokumente von file://c:/www-data/intranet/docs indiziert und im
  Browser als http://intranet.murfman.de/docs angezeigt werden.
* Crawler: Zusatzfelder: Der Index kann durch Zusatzfelder erweitert werden, die
  aus der URL eines Dokuments generiert werden.
  Beispiel: Angenommen Sie haben ein Verzeichnis mit einem Unterverzeichnis fuer
  jedes Projekt. Dann koennten Sie daraus ein Feld mit dem Projektnamen
  generieren. Dadurch bekommen Sie bei der Suche nach "Angebotï¿½project:otto23"
  nur Treffer aus diesem Verzeichnis.
* Suche: Expertensuche: Die Werte, die fuer ein Feld im Index stehen,
  koennen nun als ComboBox auf einer Suchseite angeboten werden. Vor allem in
  Verbindung mit Zusatzfeldern ist das sehr praktisch.
* Suche: Weil manche Browser aus Sicherheitsgruenden keine file-Links
  verfolgen, die auf http-Seiten stehen, sind nun alle Dokumente, die sich im
  Index befinden auch ueber HTTP erreichbar. Selbstverstaendlich sind sie bei der
  Desktopsuche nur vom lokalen Rechner aus abrufbar.
* Crawler: Der JacobMsWordPraeparator beruecksichtigt nun Formatvorlagen.
  Dadurch koennen Ueberschriften extrahiert werden, die dann bei der Suche
  staerker gewichtet werden.
* Crawler: Die JacobMsOfficePraeparatoren koennen nun die Beschreibungsfelder
  von MS Office-Dokumenten extrahieren (Titel, Autor, usw.)


Version 1.0 am 10.06.04
-----------------------

* Erste Version

